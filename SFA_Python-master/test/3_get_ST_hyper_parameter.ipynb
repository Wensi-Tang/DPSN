{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECG200\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:504: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3828: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tangw/.conda/envs/python35/lib/python3.5/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.cluster.k_means_ module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.cluster. Anything that cannot be imported from sklearn.cluster is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/optimizers.py:744: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:3066: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:166: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:171: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:176: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:180: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:189: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:196: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/tangw/.conda/envs/python35/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py:960: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tangw/.conda/envs/python35/lib/python3.5/site-packages/tslearn/shapelets.py:172: RuntimeWarning: invalid value encountered in log10\n",
      "  n_shapelets = int(numpy.log10(n_ts * (ts_sz - shp_sz + 1) * (n_classes - 1)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in  12\n",
      "best shapelet_length for dataset ECG200 is 8\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "import pandas as pd\n",
    "from os.path import dirname\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from keras.optimizers import Adagrad\n",
    "import matplotlib.pyplot as plt\n",
    "from tslearn.datasets import CachedDatasets\n",
    "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
    "from tslearn.shapelets import ShapeletModel, grabocka_params_to_shapelet_size_dict\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def covert_list_to_dic(XX_list):\n",
    "    result_dic ={}\n",
    "    for i,item in enumerate(XX_list):\n",
    "        result_dic[item]=i\n",
    "    return result_dic\n",
    "\n",
    "def load_st_result(name, ratio_number_list, ind_number_list  = [0,1,2,3,4,5,6,7,8,9]):\n",
    "    father_path = './ST_acc_log/' + name\n",
    "    path = father_path + '/' + name + '_log.txt'\n",
    "    ratio_dic = covert_list_to_dic(ratio_number_list)\n",
    "    ind_dic = covert_list_to_dic(ind_number_list)\n",
    "    proto_result = np.zeros([len(ratio_number_list), len(ind_number_list)])\n",
    "    try:\n",
    "        f = open(path, \"r\")\n",
    "        for x in f:\n",
    "            temp = x.split('\\t')\n",
    "            if float(temp[3]) == 1:\n",
    "                proto_result[-1:, :] = np.ones((1, len(ind_number_list))) * float(temp[-1])\n",
    "                continue\n",
    "\n",
    "            proto_result[ratio_dic[float(temp[3])]][ind_dic[float(temp[5])]] = float(temp[-1])\n",
    "    except:\n",
    "        print(name, 'cannot find')\n",
    "\n",
    "    return proto_result\n",
    "\n",
    "\n",
    "def save_hyper_parameter_to_log(sentence, dataset_name):\n",
    "    father_path = './ST_hyper_parameter_log/' + dataset_name\n",
    "    if os.path.exists(father_path):\n",
    "        shutil.rmtree(father_path)\n",
    "    if not os.path.exists(father_path):\n",
    "        os.makedirs(father_path)\n",
    "    path = father_path + '/' + dataset_name + '_log.txt'\n",
    "    with open(path, \"a\") as myfile:\n",
    "        myfile.write(sentence + '\\n')\n",
    "\n",
    "def load_hyper_parameter_log(dataset_name):\n",
    "    father_path = './ST_hyper_parameter_log/' + dataset_name\n",
    "    path = father_path + '/' + dataset_name + '_log.txt'\n",
    "    try:\n",
    "        f = open(path, \"r\")\n",
    "        for x in f:\n",
    "            return str(x)\n",
    "    except:\n",
    "        print(name, 'cannot find')\n",
    "\n",
    "\n",
    "\n",
    "def save_to_log(sentence, dataset_name):\n",
    "    father_path = './ST_acc_log/' + dataset_name\n",
    "    if not os.path.exists(father_path):\n",
    "        os.makedirs(father_path)\n",
    "    path = father_path + '/' + dataset_name + '_log.txt'\n",
    "    with open(path, \"a\") as myfile:\n",
    "        myfile.write(sentence + '\\n')\n",
    "\n",
    "def TSC_data_loader(dataset_name):\n",
    "    Train_dataset = np.loadtxt(\n",
    "        dirname(os.getcwd()) + '/datasets/UCRArchive_2018/' + dataset_name + '/' + dataset_name + '_TRAIN.tsv')\n",
    "    Test_dataset = np.loadtxt(\n",
    "        dirname(os.getcwd()) + '/datasets/UCRArchive_2018/' + dataset_name + '/' + dataset_name + '_TEST.tsv')\n",
    "    Train_dataset = Train_dataset.astype(np.float32)\n",
    "    Test_dataset = Test_dataset.astype(np.float32)\n",
    "\n",
    "    X_train = Train_dataset[:, 1:]\n",
    "    y_train = Train_dataset[:, 0:1]\n",
    "\n",
    "    X_test = Test_dataset[:, 1:]\n",
    "    y_test = Test_dataset[:, 0:1]\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(np.squeeze(y_train, axis=1))\n",
    "    y_train = le.transform(np.squeeze(y_train, axis=1))\n",
    "    y_test = le.transform(np.squeeze(y_test, axis=1))\n",
    "\n",
    "    # X_train = np.nan_to_num(X_train)\n",
    "    # X_test  = np.nan_to_num(X_test)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "\n",
    "def shapelet_best_predict(X_train, y_train, X_test, y_test):\n",
    "    X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
    "    X_test = TimeSeriesScalerMinMax().fit_transform(X_test)\n",
    "    \n",
    "    r_list = [2,4,6,8,12]\n",
    "    result_list = [0 for i in range(len(r_list))]\n",
    "    for r_number_index, r_number in enumerate(r_list):\n",
    "        try:\n",
    "            shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=X_train.shape[0],\n",
    "                                                       ts_sz=X_train.shape[1],\n",
    "                                                       n_classes=len(set(y_train)),\n",
    "                                                       l=0.1,\n",
    "                                                       r=r_number)\n",
    "\n",
    "            shp_clf = ShapeletModel(n_shapelets_per_size=shapelet_sizes,\n",
    "                            optimizer=Adagrad(lr=.1),\n",
    "                            weight_regularizer=.01,\n",
    "                            max_iter=50,\n",
    "                            verbose_level=0)\n",
    "            shp_clf.fit(X_train, y_train)\n",
    "            y_predict = shp_clf.predict(X_train)\n",
    "            result_list[r_number_index] = accuracy_score(y_predict,y_train)\n",
    "        except:\n",
    "            print(\"error in \",r_number)\n",
    "    \n",
    "    best_r_ind =np.argmax(result_list)  \n",
    "    shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=X_train.shape[0],\n",
    "                                                       ts_sz=X_train.shape[1],\n",
    "                                                       n_classes=len(set(y_train)),\n",
    "                                                       l=0.1,\n",
    "                                                       r=r_list[best_r_ind])\n",
    "    \n",
    "    shp_clf.fit(X_train, y_train)\n",
    "    y_predict = shp_clf.predict(X_test)\n",
    "    result = accuracy_score(y_predict,y_test)\n",
    "    return result , r_list[best_r_ind]\n",
    "    \n",
    "\n",
    "def shapelet_predict(X_train, y_train, X_test, y_test,r_number):\n",
    "    X_train = TimeSeriesScalerMinMax().fit_transform(X_train)\n",
    "    X_test = TimeSeriesScalerMinMax().fit_transform(X_test)\n",
    "    shapelet_sizes = grabocka_params_to_shapelet_size_dict(n_ts=X_train.shape[0],\n",
    "                                                       ts_sz=X_train.shape[1],\n",
    "                                                       n_classes=len(set(y_train)),\n",
    "                                                       l=0.1,\n",
    "                                                       r=r_number)\n",
    "\n",
    "    shp_clf = ShapeletModel(n_shapelets_per_size=shapelet_sizes,\n",
    "                            optimizer=Adagrad(lr=.1),\n",
    "                            weight_regularizer=.01,\n",
    "                            max_iter=50,\n",
    "                            verbose_level=0)\n",
    "    shp_clf.fit(X_train, y_train)\n",
    "    y_predict = shp_clf.predict(X_test)\n",
    "    acc = accuracy_score(y_predict,y_test)\n",
    "    \n",
    "    return acc \n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(0) \n",
    "name_list = [\n",
    "    'ECG200',\n",
    "]    \n",
    "\n",
    "train_ratio_list = [0.1,0.2,0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "for name in name_list:\n",
    "    print(name)\n",
    "    X_train_ori, y_train_ori, X_test, y_test= TSC_data_loader(name)\n",
    "    result , shapelet_number = shapelet_best_predict(X_train_ori, y_train_ori, X_test, y_test)\n",
    "    save_hyper_parameter_to_log(str(shapelet_number), name)\n",
    "    print('best shapelet_length for dataset',name, 'is', shapelet_number)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
